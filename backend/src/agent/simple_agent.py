"""SimpleAgent - æ”¯æŒæµå¼å·¥å…·è°ƒç”¨çš„ Simple Agent"""

import json
from datetime import datetime
from typing import Optional, List, Dict, AsyncGenerator, TYPE_CHECKING, Union

from hello_agents.core.agent import Agent
from hello_agents.core.llm import HelloAgentsLLM
from hello_agents.core.config import Config
from hello_agents.core.message import Message
from hello_agents.core.streaming import StreamEvent, StreamEventType

# å¯¼å…¥æœ¬åœ°å¢å¼ºç‰ˆ LLMï¼ˆæ”¯æŒæµå¼å·¥å…·è°ƒç”¨ï¼‰
from .llm import EnhancedLLM, StreamToolEventType, StreamToolCallResult

if TYPE_CHECKING:
    from hello_agents.tools.registry import ToolRegistry


class SimpleAgent(Agent):
    """æ”¯æŒæµå¼å·¥å…·è°ƒç”¨çš„ SimpleAgent

    ç‰¹æ€§ï¼š
    - çº¯å¯¹è¯æ¨¡å¼ï¼ˆæ— å·¥å…·ï¼‰
    - Function Calling å·¥å…·è°ƒç”¨ï¼ˆå¯é€‰ï¼‰
    - è‡ªåŠ¨å¤šè½®å·¥å…·è°ƒç”¨
    - çœŸæ­£çš„æµå¼è¾“å‡ºï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰

    Note:
        æ¨èä½¿ç”¨ EnhancedLLM ä»¥è·å¾—å®Œæ•´çš„æµå¼å·¥å…·è°ƒç”¨æ”¯æŒã€‚
        å¦‚æœä½¿ç”¨æ™®é€š HelloAgentsLLMï¼Œæµå¼å·¥å…·è°ƒç”¨å°†å›é€€åˆ°éæµå¼æ¨¡å¼ã€‚
    """

    def __init__(
        self,
        name: str,
        llm: Union[HelloAgentsLLM, EnhancedLLM],
        system_prompt: Optional[str] = None,
        config: Optional[Config] = None,
        tool_registry: Optional['ToolRegistry'] = None,
        enable_tool_calling: bool = True,
        max_tool_iterations: int = 10,
    ):
        """åˆå§‹åŒ– SimpleAgent

        Args:
            name: Agent åç§°
            llm: LLM å®ä¾‹ï¼ˆæ¨èä½¿ç”¨ EnhancedLLMï¼‰
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            config: é…ç½®å¯¹è±¡
            tool_registry: å·¥å…·æ³¨å†Œè¡¨ï¼ˆå¯é€‰ï¼‰
            enable_tool_calling: æ˜¯å¦å¯ç”¨å·¥å…·è°ƒç”¨
            max_tool_iterations: æœ€å¤§å·¥å…·è°ƒç”¨è¿­ä»£æ¬¡æ•°
        """
        super().__init__(
            name,
            llm,
            system_prompt,
            config,
            tool_registry=tool_registry
        )
        self.enable_tool_calling = enable_tool_calling and tool_registry is not None
        self.max_tool_iterations = max_tool_iterations

        # æ£€æŸ¥æ˜¯å¦æ”¯æŒæµå¼å·¥å…·è°ƒç”¨
        self._supports_streaming_tools = isinstance(llm, EnhancedLLM)

    def run(self, input_text: str, **kwargs) -> str:
        """åŒæ­¥è¿è¡Œï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰

        Args:
            input_text: ç”¨æˆ·è¾“å…¥
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            æœ€ç»ˆå›å¤
        """
        session_start_time = datetime.now()

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = self._build_messages(input_text)

        print(f"\nğŸ¤– {self.name} å¼€å§‹å¤„ç†é—®é¢˜: {input_text}")

        # å¦‚æœæ²¡æœ‰å¯ç”¨å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å› LLM å“åº”
        if not self.enable_tool_calling or not self.tool_registry:
            print("ğŸ“ çº¯å¯¹è¯æ¨¡å¼ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰")
            llm_response = self.llm.invoke(messages, **kwargs)
            response_text = llm_response.content if hasattr(llm_response, 'content') else str(llm_response)

            # ä¿å­˜åˆ°å†å²è®°å½•
            self.add_message(Message(input_text, "user"))
            self.add_message(Message(response_text, "assistant"))

            print(f"ğŸ’¬ å›å¤: {response_text[:100]}..." if len(response_text) > 100 else f"ğŸ’¬ å›å¤: {response_text}")
            return response_text

        # å¯ç”¨å·¥å…·è°ƒç”¨æ¨¡å¼
        tool_schemas = self._build_tool_schemas()
        print(f"ğŸ”§ å·²å¯ç”¨å·¥å…·è°ƒç”¨ï¼Œå¯ç”¨å·¥å…·: {list(self.tool_registry._tools.keys())}")

        current_iteration = 0
        final_response = ""

        while current_iteration < self.max_tool_iterations:
            current_iteration += 1
            print(f"\n--- ç¬¬ {current_iteration} è½® ---")

            # è°ƒç”¨ LLMï¼ˆFunction Callingï¼‰
            try:
                response = self.llm.invoke_with_tools(
                    messages=messages,
                    tools=tool_schemas,
                    tool_choice="auto",
                    **kwargs
                )
            except Exception as e:
                print(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
                break

            # è·å–å“åº”æ¶ˆæ¯
            response_message = response.choices[0].message

            # æ‰“å° LLM è¾“å‡º
            if response_message.content:
                content_preview = response_message.content[:200] + "..." if len(response_message.content) > 200 else response_message.content
                print(f"ğŸ’­ LLM è¾“å‡º: {content_preview}")

            # å¤„ç†å·¥å…·è°ƒç”¨
            tool_calls = response_message.tool_calls
            if not tool_calls:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›æ–‡æœ¬å“åº”
                final_response = response_message.content or "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"
                print(f"ğŸ’¬ ç›´æ¥å›å¤: {final_response[:100]}..." if len(final_response) > 100 else f"ğŸ’¬ ç›´æ¥å›å¤: {final_response}")
                break

            print(f"ğŸ”§ å‡†å¤‡æ‰§è¡Œ {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨...")

            # å°†åŠ©æ‰‹æ¶ˆæ¯æ·»åŠ åˆ°å†å²
            messages.append({
                "role": "assistant",
                "content": response_message.content,
                "tool_calls": [
                    {
                        "id": tc.id,
                        "type": "function",
                        "function": {
                            "name": tc.function.name,
                            "arguments": tc.function.arguments
                        }
                    }
                    for tc in tool_calls
                ]
            })

            # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
            for tool_call in tool_calls:
                tool_name = tool_call.function.name
                tool_call_id = tool_call.id

                try:
                    arguments = json.loads(tool_call.function.arguments)
                except json.JSONDecodeError as e:
                    print(f"âŒ å·¥å…·å‚æ•°è§£æå¤±è´¥: {e}")
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call_id,
                        "content": f"é”™è¯¯ï¼šå‚æ•°æ ¼å¼ä¸æ­£ç¡® - {str(e)}"
                    })
                    continue

                print(f"ğŸ¬ è°ƒç”¨å·¥å…·: {tool_name}({arguments})")

                # æ‰§è¡Œå·¥å…·ï¼ˆå¤ç”¨åŸºç±»æ–¹æ³•ï¼‰
                result = self._execute_tool_call(tool_name, arguments)

                # æˆªæ–­æ˜¾ç¤º
                result_preview = result[:200] + "..." if len(result) > 200 else result
                if result.startswith("âŒ"):
                    print(f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {result_preview}")
                else:
                    print(f"ğŸ‘€ è§‚å¯Ÿ: {result_preview}")

                # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call_id,
                    "content": result
                })

        # å¦‚æœè¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œè·å–æœ€åä¸€æ¬¡å›ç­”
        if current_iteration >= self.max_tool_iterations and not final_response:
            print("â° å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œè·å–æœ€ç»ˆå›ç­”...")
            llm_response = self.llm.invoke(messages, **kwargs)
            final_response = llm_response.content if hasattr(llm_response, 'content') else str(llm_response)

        # ä¿å­˜åˆ°å†å²è®°å½•
        self.add_message(Message(input_text, "user"))
        self.add_message(Message(final_response, "assistant"))

        duration = (datetime.now() - session_start_time).total_seconds()
        print(f"\nâœ… å®Œæˆï¼Œè€—æ—¶ {duration:.2f}sï¼Œå…± {current_iteration} è½®")

        return final_response

    def _build_messages(self, input_text: str) -> List[Dict[str, str]]:
        """æ„å»ºæ¶ˆæ¯åˆ—è¡¨"""
        messages = []

        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        if self.system_prompt:
            messages.append({
                "role": "system",
                "content": self.system_prompt
            })

        # æ·»åŠ å†å²æ¶ˆæ¯
        for msg in self._history:
            messages.append({
                "role": msg.role,
                "content": msg.content
            })

        # æ·»åŠ ç”¨æˆ·é—®é¢˜
        messages.append({
            "role": "user",
            "content": input_text
        })

        print(f"ğŸ“¦ æ„å»ºæ¶ˆæ¯: {len(messages)} æ¡ï¼ˆç³»ç»Ÿ: {1 if self.system_prompt else 0}, å†å²: {len(self._history)}, ç”¨æˆ·: 1ï¼‰")

        return messages

    async def arun_stream(
        self,
        input_text: str,
        **kwargs
    ) -> AsyncGenerator[StreamEvent, None]:
        """å¼‚æ­¥æµå¼è¿è¡Œï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰

        ä½¿ç”¨ LLM çš„ astream_invoke_with_tools æ–¹æ³•å®ç°ä¼˜é›…çš„æµå¼å·¥å…·è°ƒç”¨ã€‚

        Args:
            input_text: ç”¨æˆ·è¾“å…¥
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            StreamEvent: æµå¼äº‹ä»¶
        """
        session_start_time = datetime.now()

        # å‘é€å¼€å§‹äº‹ä»¶
        yield StreamEvent.create(
            StreamEventType.AGENT_START,
            self.name,
            input_text=input_text
        )

        print(f"\nğŸ¤– {self.name} å¼€å§‹å¤„ç†é—®é¢˜ï¼ˆæµå¼ï¼‰: {input_text}")

        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = self._build_messages(input_text)

            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·
            if not self.enable_tool_calling or not self.tool_registry:
                # çº¯å¯¹è¯æ¨¡å¼
                async for event in self._stream_without_tools(messages, **kwargs):
                    yield event
                return

            # æ£€æŸ¥ LLM æ˜¯å¦æ”¯æŒæµå¼å·¥å…·è°ƒç”¨
            if not self._supports_streaming_tools:
                import warnings
                warnings.warn(
                    "å½“å‰ LLM ä¸æ”¯æŒæµå¼å·¥å…·è°ƒç”¨ï¼Œå°†ä½¿ç”¨éæµå¼æ¨¡å¼ã€‚"
                    "æ¨èä½¿ç”¨ EnhancedLLM ä»¥è·å¾—æ›´å¥½çš„ä½“éªŒã€‚",
                    UserWarning
                )
                # å›é€€åˆ°åŒæ­¥æ¨¡å¼
                response = self.run(input_text, **kwargs)
                yield StreamEvent.create(
                    StreamEventType.AGENT_FINISH,
                    self.name,
                    result=response
                )
                return

            # === æµå¼å·¥å…·è°ƒç”¨æ¨¡å¼ ===
            tool_schemas = self._build_tool_schemas()
            print(f"ğŸ”§ å·²å¯ç”¨å·¥å…·è°ƒç”¨ï¼Œå¯ç”¨å·¥å…·: {list(self.tool_registry._tools.keys())}")

            current_iteration = 0
            final_response = ""
            # æ”¶é›†å·¥å…·è°ƒç”¨è®°å½•ï¼ˆç”¨äºå­˜å…¥ä¼šè¯ï¼‰
            tool_call_records: List[Dict[str, Any]] = []

            while current_iteration < self.max_tool_iterations:
                current_iteration += 1

                # å‘é€æ­¥éª¤å¼€å§‹äº‹ä»¶
                yield StreamEvent.create(
                    StreamEventType.STEP_START,
                    self.name,
                    step=current_iteration,
                    max_steps=self.max_tool_iterations
                )

                print(f"\n--- ç¬¬ {current_iteration} è½® ---")
                print("ğŸ’­ LLM è¾“å‡º: ", end="", flush=True)

                # ä½¿ç”¨ LLM çš„æµå¼å·¥å…·è°ƒç”¨æ–¹æ³•
                try:
                    async for event in self.llm.astream_invoke_with_tools(
                        messages=messages,
                        tools=tool_schemas,
                        tool_choice="auto",
                        **kwargs
                    ):
                        # å¤„ç†æ–‡æœ¬å†…å®¹
                        if event.event_type == StreamToolEventType.CONTENT:
                            yield StreamEvent.create(
                                StreamEventType.LLM_CHUNK,
                                self.name,
                                chunk=event.content,
                                step=current_iteration
                            )
                            print(event.content, end="", flush=True)

                        # å·¥å…·è°ƒç”¨å¼€å§‹ï¼ˆæ‰“å°ä¿¡æ¯ï¼Œä¸å‘é€äº‹ä»¶ï¼‰
                        elif event.event_type == StreamToolEventType.TOOL_CALL_START:
                            pass  # ç­‰å·¥å…·è°ƒç”¨å®Œæˆåå†å‘é€äº‹ä»¶

                    print()  # æ¢è¡Œ

                except Exception as e:
                    error_msg = f"LLM è°ƒç”¨å¤±è´¥: {str(e)}"
                    print(f"\nâŒ {error_msg}")
                    yield StreamEvent.create(
                        StreamEventType.ERROR,
                        self.name,
                        error=error_msg
                    )
                    break

                # è·å–ç´¯ç§¯ç»“æœ
                result = self.llm.get_last_stream_tool_result()
                if result is None:
                    break

                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                complete_tool_calls = result.get_complete_tool_calls()

                # æ— è®ºæ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ï¼Œéƒ½ä¿å­˜æœ¬è½®çš„æ–‡æœ¬å†…å®¹
                if result.content:
                    final_response = result.content

                if not complete_tool_calls:
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›
                    if not final_response:
                        final_response = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"
                    # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
                    preview = final_response[:100] + "..." if len(final_response) > 100 else final_response
                    print(f"ğŸ’¬ ç›´æ¥å›å¤: {preview}")
                    break

                print(f"ğŸ”§ å‡†å¤‡æ‰§è¡Œ {len(complete_tool_calls)} ä¸ªå·¥å…·è°ƒç”¨...")

                # å°†åŠ©æ‰‹æ¶ˆæ¯æ·»åŠ åˆ°å†å²
                messages.append(result.to_assistant_message())

                # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
                for tc in complete_tool_calls:
                    tool_name = tc["name"]
                    tool_call_id = tc["id"]

                    try:
                        arguments = json.loads(tc["arguments"])
                    except json.JSONDecodeError as e:
                        print(f"âŒ å·¥å…·å‚æ•°è§£æå¤±è´¥: {e}")
                        messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call_id,
                            "content": f"é”™è¯¯ï¼šå‚æ•°æ ¼å¼ä¸æ­£ç¡® - {str(e)}"
                        })
                        continue

                    print(f"ğŸ¬ è°ƒç”¨å·¥å…·: {tool_name}({arguments})")

                    # å‘é€å·¥å…·è°ƒç”¨å¼€å§‹äº‹ä»¶
                    yield StreamEvent.create(
                        StreamEventType.TOOL_CALL_START,
                        self.name,
                        tool_name=tool_name,
                        tool_call_id=tool_call_id,
                        args=arguments
                    )

                    # æ‰§è¡Œå·¥å…·
                    exec_result = self._execute_tool_call(tool_name, arguments)

                    # æˆªæ–­æ˜¾ç¤º
                    result_preview = exec_result[:200] + "..." if len(exec_result) > 200 else exec_result
                    if exec_result.startswith("âŒ"):
                        print(f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {result_preview}")
                    else:
                        print(f"ğŸ‘€ è§‚å¯Ÿ: {result_preview}")

                    # å‘é€å·¥å…·è°ƒç”¨å®Œæˆäº‹ä»¶
                    yield StreamEvent.create(
                        StreamEventType.TOOL_CALL_FINISH,
                        self.name,
                        tool_name=tool_name,
                        tool_call_id=tool_call_id,
                        result=exec_result
                    )

                    # è®°å½•å·¥å…·è°ƒç”¨ï¼ˆç”¨äºå­˜å…¥ä¼šè¯ï¼‰
                    tool_call_records.append({
                        "name": tool_name,
                        "args": arguments,
                        "result": exec_result,
                        "status": "error" if exec_result.startswith("âŒ") else "done"
                    })

                    # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call_id,
                        "content": exec_result
                    })

                # å‘é€æ­¥éª¤å®Œæˆäº‹ä»¶
                yield StreamEvent.create(
                    StreamEventType.STEP_FINISH,
                    self.name,
                    step=current_iteration
                )

            # å¦‚æœè¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œè·å–æœ€åä¸€æ¬¡å›ç­”
            if current_iteration >= self.max_tool_iterations and not final_response:
                print("â° å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œè·å–æœ€ç»ˆå›ç­”...")

                try:
                    async for chunk in self.llm.astream_invoke(messages, **kwargs):
                        final_response += chunk
                        yield StreamEvent.create(
                            StreamEventType.LLM_CHUNK,
                            self.name,
                            chunk=chunk
                        )
                        print(chunk, end="", flush=True)
                    print()
                except Exception as e:
                    print(f"âŒ æœ€ç»ˆå›ç­”å¤±è´¥: {e}")
                    result = self.llm.get_last_stream_tool_result()
                    final_response = result.content if result else "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"

            # ä¿å­˜åˆ°å†å²è®°å½•ï¼ˆæŒ‰ç…§ OpenAI è§„èŒƒæ ¼å¼ï¼‰
            self.add_message(Message(input_text, "user"))

            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œä¿å­˜å·¥å…·è°ƒç”¨æ¶ˆæ¯
            if tool_call_records:
                # ä¿å­˜ assistant æ¶ˆæ¯ï¼ˆåŒ…å« tool_callsï¼‰
                tool_calls_for_message = [
                    {
                        "id": f"call_{i}",
                        "type": "function",
                        "function": {
                            "name": tc["name"],
                            "arguments": json.dumps(tc["args"])
                        }
                    }
                    for i, tc in enumerate(tool_call_records)
                ]
                self.add_message(Message(
                    "",  # å·¥å…·è°ƒç”¨æ—¶å¯èƒ½æ²¡æœ‰æ–‡æœ¬å†…å®¹
                    "assistant",
                    metadata={"tool_calls": tool_calls_for_message}
                ))

                # ä¿å­˜æ¯ä¸ª tool æ¶ˆæ¯
                for i, tc in enumerate(tool_call_records):
                    self.add_message(Message(
                        tc["result"],
                        "tool",
                        metadata={"tool_call_id": f"call_{i}"}
                    ))

            # ä¿å­˜æœ€ç»ˆ assistant å›ç­”
            if final_response:
                self.add_message(Message(final_response, "assistant"))

            duration = (datetime.now() - session_start_time).total_seconds()
            print(f"\nâœ… å®Œæˆï¼Œè€—æ—¶ {duration:.2f}sï¼Œå…± {current_iteration} è½®")

            # å‘é€å®Œæˆäº‹ä»¶
            yield StreamEvent.create(
                StreamEventType.AGENT_FINISH,
                self.name,
                result=final_response
            )

        except Exception as e:
            print(f"âŒ Agent æ‰§è¡Œå¤±è´¥: {e}")
            yield StreamEvent.create(
                StreamEventType.ERROR,
                self.name,
                error=str(e),
                error_type=type(e).__name__
            )
            # ä¸è¦ raiseï¼Œç¡®ä¿æµå¼å“åº”æ­£å¸¸ç»“æŸ
            # å‘é€å®Œæˆäº‹ä»¶ä»¥ä¼˜é›…ç»“æŸ
            yield StreamEvent.create(
                StreamEventType.AGENT_FINISH,
                self.name,
                result=""  # ç©ºç»“æœè¡¨ç¤ºå¤±è´¥
            )

    async def _stream_without_tools(
        self,
        messages: List[Dict],
        **kwargs
    ) -> AsyncGenerator[StreamEvent, None]:
        """çº¯å¯¹è¯æ¨¡å¼ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰"""
        print("ğŸ“ çº¯å¯¹è¯æ¨¡å¼ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰")

        full_response = ""
        async for chunk in self.llm.astream_invoke(messages, **kwargs):
            full_response += chunk
            yield StreamEvent.create(
                StreamEventType.LLM_CHUNK,
                self.name,
                chunk=chunk
            )
            print(chunk, end="", flush=True)

        print()

        # ä¿å­˜å†å²
        self.add_message(Message(messages[-1]["content"], "user"))
        self.add_message(Message(full_response, "assistant"))

        print(f"ğŸ’¬ å›å¤å®Œæˆ")

        yield StreamEvent.create(
            StreamEventType.AGENT_FINISH,
            self.name,
            result=full_response
        )
